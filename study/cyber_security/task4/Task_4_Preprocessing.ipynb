{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импортирование необходимых модулей\n",
    "\n",
    "Для модуля NLTK необходима дополнительная загрузка данных для функций токенизации, лемматизации и для корпуса стоп-слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nshir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nshir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nshir\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize # см. ниже\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from joblib import Parallel, delayed # см. ниже\n",
    "from unidecode import unidecode # см. ниже\n",
    "from datetime import datetime # см. ниже\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка датасета из CSV-файла\n",
    "\n",
    "Файл должен находиться в той же папке, что и блокнот. В противном случае, необходимо указать к нему путь. Предоставленный файл содержит хедер с названиями колонок. Содержание текстовых строк соответствует кодировке ISO/IEC 8859-1, или Latin-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>date</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>subject</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2002-03-15T17:06:22</td>\n",
       "      <td>info@global-change.com</td>\n",
       "      <td>michelle.lokay@enron.com</td>\n",
       "      <td>Next Wave of Energy Trading</td>\n",
       "      <td>Energy Industry Professional: Â  Global Chang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2002-01-29T11:16:35</td>\n",
       "      <td>info@pmaconference.com</td>\n",
       "      <td>michelle.lokay@enron.com</td>\n",
       "      <td>Register for the Next TXU Capacity Auction!</td>\n",
       "      <td>Register for the next TXU Energy Capacity Auct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2002-01-27T22:06:53</td>\n",
       "      <td>info@pmaconference.com</td>\n",
       "      <td>michelle.lokay@enron.com</td>\n",
       "      <td>Merchant Power Monthly   Free Sample</td>\n",
       "      <td>.................................................</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2002-03-14T07:22:17</td>\n",
       "      <td>bruno@firstconf.com</td>\n",
       "      <td>energynews@fc.ease.lsoft.com</td>\n",
       "      <td>t,h: Eyeforenergy Update</td>\n",
       "      <td>Welcome to this week's Eyeforenergy Update.  J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2002-03-05T03:51:53</td>\n",
       "      <td>deanrogers@energyclasses.com</td>\n",
       "      <td>michelle.lokay@enron.com</td>\n",
       "      <td>Derivatives Early Bird 'til March 11, Houston</td>\n",
       "      <td>Derivatives For Energy Professionals   Two Ful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31710</th>\n",
       "      <td>1</td>\n",
       "      <td>2004-06-24T04:51:30</td>\n",
       "      <td>jacob rzucidlo &lt;lavoneaker@stalag13.com&gt;</td>\n",
       "      <td>johnny wynott &lt;varou@iit.demokritos.gr&gt;</td>\n",
       "      <td>Cpu PAIN M.edICATI0N... SHIPPED to Your D00R !</td>\n",
       "      <td>\\n\\n arrghh  wests  amnstv  amlsmith  basus\\nÂ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31711</th>\n",
       "      <td>1</td>\n",
       "      <td>2004-06-19T23:52:54</td>\n",
       "      <td>hal leake &lt;annettgaskell@buglover.net&gt;</td>\n",
       "      <td>renato mooney &lt;sigletos@iit.demokritos.gr&gt;</td>\n",
       "      <td>Dn trouble f.r.ee</td>\n",
       "      <td>\\n\\n\\n\\n\\nDn trouble f.r.ee\\n\\n\\n\\nangiospasmÂ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31712</th>\n",
       "      <td>1</td>\n",
       "      <td>2004-06-30T06:07:33</td>\n",
       "      <td>dr collins khumalo &lt;khumalo_20@sunumail.sn&gt;</td>\n",
       "      <td>khumalo_20@sunumail.sn</td>\n",
       "      <td>Dr.Collins Khumalo.</td>\n",
       "      <td>\\n\\n\\n\\n\\nDr.Collins Khumalo.\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31713</th>\n",
       "      <td>1</td>\n",
       "      <td>2004-10-10T12:00:18</td>\n",
       "      <td>Customer Support &lt;support@citibank.com&gt;</td>\n",
       "      <td>Paliourg &lt;paliourg@iit.demokritos.gr&gt;</td>\n",
       "      <td>Dear customer your details have been compromised</td>\n",
       "      <td>\\n\\n\\n\\n\\nDear customer your details have been...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31714</th>\n",
       "      <td>1</td>\n",
       "      <td>2004-12-26T12:34:43</td>\n",
       "      <td>Tapanga Cribbin &lt;James_Lam@cnwl.igs.net&gt;</td>\n",
       "      <td>paliourg@iit.demokritos.gr</td>\n",
       "      <td>Fwd: Great news</td>\n",
       "      <td>\\n\\nNo state shall, without the consent of the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30692 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       class                 date  \\\n",
       "0          0  2002-03-15T17:06:22   \n",
       "1          0  2002-01-29T11:16:35   \n",
       "2          0  2002-01-27T22:06:53   \n",
       "3          0  2002-03-14T07:22:17   \n",
       "4          0  2002-03-05T03:51:53   \n",
       "...      ...                  ...   \n",
       "31710      1  2004-06-24T04:51:30   \n",
       "31711      1  2004-06-19T23:52:54   \n",
       "31712      1  2004-06-30T06:07:33   \n",
       "31713      1  2004-10-10T12:00:18   \n",
       "31714      1  2004-12-26T12:34:43   \n",
       "\n",
       "                                              from  \\\n",
       "0                           info@global-change.com   \n",
       "1                           info@pmaconference.com   \n",
       "2                           info@pmaconference.com   \n",
       "3                              bruno@firstconf.com   \n",
       "4                     deanrogers@energyclasses.com   \n",
       "...                                            ...   \n",
       "31710     jacob rzucidlo <lavoneaker@stalag13.com>   \n",
       "31711       hal leake <annettgaskell@buglover.net>   \n",
       "31712  dr collins khumalo <khumalo_20@sunumail.sn>   \n",
       "31713      Customer Support <support@citibank.com>   \n",
       "31714     Tapanga Cribbin <James_Lam@cnwl.igs.net>   \n",
       "\n",
       "                                               to  \\\n",
       "0                        michelle.lokay@enron.com   \n",
       "1                        michelle.lokay@enron.com   \n",
       "2                        michelle.lokay@enron.com   \n",
       "3                    energynews@fc.ease.lsoft.com   \n",
       "4                        michelle.lokay@enron.com   \n",
       "...                                           ...   \n",
       "31710     johnny wynott <varou@iit.demokritos.gr>   \n",
       "31711  renato mooney <sigletos@iit.demokritos.gr>   \n",
       "31712                      khumalo_20@sunumail.sn   \n",
       "31713       Paliourg <paliourg@iit.demokritos.gr>   \n",
       "31714                  paliourg@iit.demokritos.gr   \n",
       "\n",
       "                                                subject  \\\n",
       "0                           Next Wave of Energy Trading   \n",
       "1           Register for the Next TXU Capacity Auction!   \n",
       "2                  Merchant Power Monthly   Free Sample   \n",
       "3                              t,h: Eyeforenergy Update   \n",
       "4         Derivatives Early Bird 'til March 11, Houston   \n",
       "...                                                 ...   \n",
       "31710    Cpu PAIN M.edICATI0N... SHIPPED to Your D00R !   \n",
       "31711                                 Dn trouble f.r.ee   \n",
       "31712                               Dr.Collins Khumalo.   \n",
       "31713  Dear customer your details have been compromised   \n",
       "31714                                   Fwd: Great news   \n",
       "\n",
       "                                                    body  \n",
       "0       Energy Industry Professional: Â  Global Chang...  \n",
       "1      Register for the next TXU Energy Capacity Auct...  \n",
       "2      .................................................  \n",
       "3      Welcome to this week's Eyeforenergy Update.  J...  \n",
       "4      Derivatives For Energy Professionals   Two Ful...  \n",
       "...                                                  ...  \n",
       "31710  \\n\\n arrghh  wests  amnstv  amlsmith  basus\\nÂ...  \n",
       "31711  \\n\\n\\n\\n\\nDn trouble f.r.ee\\n\\n\\n\\nangiospasmÂ...  \n",
       "31712  \\n\\n\\n\\n\\nDr.Collins Khumalo.\\n\\n\\n\\n\\n\\n\\n\\n\\...  \n",
       "31713  \\n\\n\\n\\n\\nDear customer your details have been...  \n",
       "31714  \\n\\nNo state shall, without the consent of the...  \n",
       "\n",
       "[30692 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Task_1_original_data.csv', newline='') as csvfile:\n",
    "    df = pd.read_csv(csvfile, encoding='latin_1')\n",
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предварительная обработка содержания\n",
    "\n",
    "Определение функции для предварительной обработки. Имеет значение порядок выполняемых операций:\n",
    "1. В первую очередь удаляются крупные элементы, т.е. URL ссылки и email адреса. Поскольку URL ссылки могут содеражать email адреса, они удаляются в первую очередь. URL ссылки могут как содержать компонент схемы (`http`, `https`, `ftp`) с разделителем (`://`), так и нет. Когда URL не содержит схему, он может быть практически не отличим от последовательности слов, разделенных точкой. Поэтому для удаления URL используется регулярное выражение с компонентом схемы во избежание удаления большого количества действительных слов. Для повторимости результатов задания необходимо использовать предоставленное регулярное выражение: `\\b[a-z]*:\\/{2}\\S*\\s*`.\n",
    "1. Регулярные выражения для email адресов, соответствующие RFC 5322 и RFC 822, слишком громоздки и неэффективны, и их использование для практического задания нецелесообразно. Для задания предоставляется простое выражение `[^@\\s]+@[^@\\s]+\\.[^@\\s]+`.\n",
    "1. Для упрощения разделения строки на слова и пунктуацию (токенизацию) используется функция `word_tokenize` из модуля `nltk`. После разделения обрабатывается отдельно каждый токен, полученный из строки.\n",
    "1. Токен может быть как словом, так и знаком пунктуации, так и комбинацией символов, в том числе плохо раскодированных при обработке датасета (например, `Õåê~úßÑ`). Для определения, является ли токен состоятельным кандидатом слова, проверяется наличие хотя бы одной буквы стандартного латинского алфавита. Данный выбор вызван тем, что слова, полностью состоящие из диакритизированных латинских букв, в европейских языках отсутствуют, или очень редки, или являются служебными. Это позволит отсеять плохо раскодированные знаки и знаки пунктуации без потери большого количества действительных слов.\n",
    "1. Символы отсеянных токенов-кандидатов конвертируются в стандартный латинский алфавит с помощью модуля `unidecode`.\n",
    "1. Последней фильтрацией полученных токенов является проверка на стоп-слова. Для этого применяется корпус стоп-слов `stopwords` из модуля `nltk`.\n",
    "1. Несмотря на то что лемматизация и вычленение корня (стемминг) — похожие методы преобразования слов, в задании предлагается использовать их совместно. В первую очередь необходимо применить лемматизацию, так как она ставит в соответствие слову лемму согласно корпусу языка, в то время как стемминг приводит слово к рудиментарному корню, не являющемуся действительным словом, на основе эвристических методов. Поэтому применение стемминга *до* лемматизация невозможно. Для обоих процедур применяется модуль `nltk`, а именно классы `WordNetLemmatizer` и `PorterStemmer`.\n",
    "\n",
    "Также необходимо обратить внимание, что для некоторых писем содержание может полностью отсутствовать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "stopwords = stopwords.words()\n",
    "\n",
    "def token_preproc(token: str):\n",
    "    if re.search(r'[a-z]', token) is None:\n",
    "        return ''\n",
    "    else:\n",
    "        # Замена диакритизированных символов стандартной латиницей\n",
    "        token = unidecode(token)\n",
    "        # Удаление стоп-слов\n",
    "        if token in stopwords:\n",
    "            return ''\n",
    "        else:\n",
    "            # Лемматизация\n",
    "            token = wnl.lemmatize(token)\n",
    "            # Стемминг\n",
    "            return ps.stem(token)\n",
    "\n",
    "def preprocess(body):\n",
    "    if not pd.isnull(body):\n",
    "        # Приведение к нижнему регистру\n",
    "        string = body.lower()\n",
    "        # Удаление URL\n",
    "        string = re.sub(r'\\b[a-z]*:\\/{2}\\S*\\s*', ' ', string)\n",
    "        # Удаление адресов электронной почты\n",
    "        string = re.sub(r'[^@\\s]+@[^@\\s]+\\.[^@\\s]+', '', string)\n",
    "        # Токенизация\n",
    "        tokens = word_tokenize(string)           \n",
    "        # Обработка токенов (см. выше)\n",
    "        tokens = [token_preproc(t) for t in tokens]\n",
    "        # Соединение токенов обратно в строку\n",
    "        string = ' '.join(tokens)\n",
    "        # Удаление оставшихся знаков пунктуации и специальных символов\n",
    "        string = re.sub(r'[^a-z]', ' ', string)\n",
    "        # Удаление повторяющихся пробелов, переносов строки, а также их удаление на концах строки\n",
    "        return re.sub(r'\\s+', ' ', string).lstrip().rstrip()\n",
    "    else:\n",
    "        # Замена отсутствующего значения пустой строкой\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описанный процесс предобработки является достаточно интенсивным ввиду множества операций lookup и проверки символов слова. Для ускорения процесса предлагается использовать средства параллелизации задач. В модельном решении используется класс `Parallel` модуля `joblib` с параметрами `n_jobs=20` (количество параллельных задач) на основе мультипроцессинга (параметр `backend='multiprocessing'`). Другие бэкенды параллелизации `loky` и `threading` работают медленнее, как показал предварительный анализ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,5] = Parallel(n_jobs=20, backend='multiprocessing')(delayed(preprocess)(str(x)) for x in df.iloc[:,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В одном из модифицированных алгоритмов из задания предлагается включить текст из темы письма (колонка `subject` в датасете), который также нуждается в предобработке. Алгоритм предобработки остается таким же, как для содержания письма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, 4] = Parallel(n_jobs=20, backend='multiprocessing')(delayed(preprocess)(str(x)) for x in df.iloc[:,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Колонка `date` датасета была предварительно предобработана и приведена к стандартному виду ISO 8601 (без указания часового пояса). Для дальнейшего использования этой колонки в одном из измененных алгоритмов метка времени преобразуется в номер дня недели, начиная с 0, с помощью модуля `datetime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    if pd.isnull(df.iat[i,1]):\n",
    "        df.iat[i,1] = 0\n",
    "    else:\n",
    "        df.iat[i,1] = datetime.strptime(df.iat[i,1], \"%Y-%m-%dT%H:%M:%S\").weekday()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сохранение датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Task_1_prepprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пользовательская обработка тестового датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\nshir\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "nltk.download('omw-1.4')\n",
    "with open(os.path.join(os.getcwd(), 'task_4_P_test_2.csv'), 'r') as file:\n",
    "    body = file.read()\n",
    "\n",
    "processed_body = preprocess(body=body)\n",
    "with open(os.path.join(os.getcwd(), 'task_4_processed_test_2.csv'), 'w') as file:\n",
    "    file.write(processed_body)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('shitenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "9da171d52472031140bc913e3da61de2d22566d9b82a055c1dbec25136f8e8ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
